{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "conditional-proceeding",
   "metadata": {},
   "source": [
    "# Spam Detection using NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abroad-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acting-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and reading the Dataset\n",
    "Dataset = pd.read_csv('SMSSpamCollection', sep='\\t', names=['label','messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-camel",
   "metadata": {},
   "source": [
    "Cleaning the dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "convenient-miniature",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mufis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing all the rquired libraries \n",
    "import re                  # This is regular expression library\n",
    "import nltk                # This is the main library for NLP\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords            # This is for removing all unwanted stopwords (Eg:in, a, etc)\n",
    "from nltk.stem.porter import PorterStemmer   # This is for stemming the words by using its common sound\n",
    "from nltk.stem import WordNetLemmatizer      # This is same as stemming but lemmatizer will give us an actual meaningful root word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "patent-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making instances of stemmer and lemmetizer\n",
    "PS = PorterStemmer()                      # We will be not using PorterStemmer in this implementation            \n",
    "Lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alpha-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning & Preprocessing (Using WordNetLemmatizer)\n",
    "Corpus = []                                                    # Putting the cleaned and preprocessed dataset inside corpus \n",
    "for i in range(0, len(Dataset)):                               # specifying the entire length of the dataset from 0 record to 5572 record\n",
    "    review = re.sub('[^a-zA-Z]', ' ', Dataset['messages'][i])  # here ^ means except any charcter between a-z and A-Z replace it with ' '(space) and do it in the messages column of the dataset with 'i' representing each record\n",
    "    review = review.lower()                                    # this will get all the words to lowercase to avoid duplicates\n",
    "    review = review.split()                                    # this will split all the words and will give us a list of words to be put in review\n",
    "    review = [Lem.lemmatize(word) for word in review if not word in stopwords.words('english')]    # here we do lemmatizing on the words present in review but omitting those words that are stopwords.words(english), so this will result in getting the base form of each non-stopword\n",
    "    review = ' '.join(review)                                  # this will join all the stemmed words (from the previous review) into sentences with ' '(spaces)\n",
    "    Corpus.append(review)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "covered-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Bag of Words model using CountVectorizer from SKLearn library\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "CV = CountVectorizer(max_features=5000)                        # Initializing and making an instance of CountVeactorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "played-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the Bag of words model into Corpus and creating the X (independent variable) with only the vectorized feature\n",
    "X = CV.fit_transform(Corpus).toarray()                         # Transforming the Corpus with CountVEctorizer and converting into an Array object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "spatial-guide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After getting our X variable from the Corpus, now we need to get the dependent Y variable from the Dataset which will basically be the Label feature\n",
    "Y = pd.get_dummies(Dataset['label'], drop_first=True)          # Since the label column was categorical (having ham and spam) we will use get_dummies encoding to convert it to binary representation of 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "architectural-decade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since Y is a dataframe, we need to convert it to an array. We will do that using iloc function and just getting the values out of it\n",
    "Y = Y.iloc[:,:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "prime-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test test for modelling\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "vulnerable-craft",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mufis\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Training the model using Naive Bayes Classifier (Since naive bayes is a classification technique that worls completely on probability, it is a good classifier for NLP)\n",
    "from sklearn.naive_bayes import MultinomialNB                  # We choose multinomial because it works for any number of classes\n",
    "MultiNB = MultinomialNB()\n",
    "spam_detection = MultiNB.fit(X_train, Y_train)                 # Fitting the NB model on the train set (x_train and y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "negative-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predicting on the test set\n",
    "Y_pred = spam_detection.predict(X_test)                        # Using the trained spam_detection model to predict values in the test set (x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dominican-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now since we have the y_pred, we need to compare the values of y_pred with the values of y_test to see how accurate our model has performed in the test set\n",
    "# For this, we will see the confusion matrix and accuracy score from sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "conf_matrix = confusion_matrix(Y_pred, Y_test)\n",
    "accuracy = accuracy_score(Y_pred, Y_test)                      # 0.9842067480258435 (98% accuracy score which is very good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "anonymous-transfer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1198   12]\n",
      " [  10  173]]\n"
     ]
    }
   ],
   "source": [
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "precise-capability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9842067480258435\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-bibliography",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
